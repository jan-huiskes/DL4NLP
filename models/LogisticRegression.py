import sys

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier

sys.path.append('../utils')

from data_loader import Dataset

class BaselineClassifier:

    """
    Calculate tf-idf on test data and classifies using logistic regression.

    Parameters
    ==========

    """

    def __init__(self, ngram_range=3, max_vocab_f=75000,
        alpha=0.0001, min_df=3, tokens_col="tweets"):

        self.vectorizer = None
        self.model = None

        # parameters for tf-idf vectorizer
        self.ngram_range = (1, ngram_range)
        self.max_vocab_f = max_vocab_f
        self.min_df = min_df
        self.tokens_col = tokens_col

        # parameter for logistic regression
        self.alpha = alpha

    def train(self, x_train, y_train):

        """
        Trains a logistic classifier.
        """

        tokenized_papers = list(x_train[self.tokens_col])

        print("Train vectrizer...")

        vec = TfidfVectorizer(ngram_range=self.ngram_range,
                            max_features=self.max_vocab_f,
                            strip_accents='unicode')

        # generate term document matrix (model inputs)
        X = vec.fit_transform(tokenized_papers)

        # TODO save vec and X

        self.vectorizer = vec

        print("Fit classifier...")

        # logistic classifier
        classifier = SGDClassifier(loss="log", alpha=self.alpha).fit(X, labels)

        # save the model
        self.model = classifier

    def predict(self, papers):

        """
        Generates predictions from the trained classifiers. Each binary
        classifier is applied once.

        Parameters
        ----------

        papers : pd.DataFrame
            papers that we want to classify. Required column:
                tokens_baseline - previously tokenized title-abstract

        Returns
        -------
        scores : pd.DataFrame
            Dataframe containing the predictions generated by each model.
            Each column corresponds to a review group and the values in
            that column are the probabilities that each paper belong to
            that review group.
        """

        scores = {}

        tokenized_papers = list(papers[self.tokens_col])

        # get vectorizer and determine tfidf for papers
        vec = self.vectorizer
        X = vec.transform(tokenized_papers)

        # get the classifier
        classifier = self.model

        # predictions as probabilities
        y_preds = classifier.predict_proba(X)

        probabilities = y_preds[:,1]

        # store scores of model
        scores[model_group] = probabilities

        scores = pd.DataFrame.from_dict(scores)

        return scores


if __name__ == '__main__':

    # load data
    dataset = Dataset("../data/cleaned_tweets_orig.csv")
    x_train, y_train, x_test, y_test = dataset.split_train_test()

    classifier = BaselineClassifier()
    classifier.train(x_train, y_train)
    scores = classifier.predict(x_test)

    precision, recall, f_score = evaluate(scores, y_test)

    print(precision)
    print(recall)
    print(f_score)
