{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitabortych/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     ï»¿index                                  count hate_speech  \\\n",
      "1            0                                      3           0   \n",
      "2            1                                      3           0   \n",
      "3            2                                      3           0   \n",
      "4            3                                      3           0   \n",
      "5            4                                      6           0   \n",
      "6            5                                      3           1   \n",
      "7            6                                      3           0   \n",
      "8            7                                      3           0   \n",
      "9            8                                      3           0   \n",
      "10           9                                      3           1   \n",
      "11       bitch  hobbies include fighting mariam bitch        None   \n",
      "12          10                                      3           0   \n",
      "13          11                                      3           0   \n",
      "14          12                                      3           0   \n",
      "15          13                                      3           0   \n",
      "16          14                                      3           1   \n",
      "17          15                                      3           0   \n",
      "18          16                                      3           0   \n",
      "19          17                                      3           1   \n",
      "20          18                                      3           0   \n",
      "21          19                                      3           0   \n",
      "22          20                                      3           0   \n",
      "23          21                                      3           0   \n",
      "24          22                                      3           0   \n",
      "25          23                                      3           0   \n",
      "26          24                                      3           0   \n",
      "27          25                                      3           0   \n",
      "28          26                                      3           0   \n",
      "29          27                                      3           0   \n",
      "30          28                                      3           0   \n",
      "...        ...                                    ...         ...   \n",
      "26368    25266                                      3           1   \n",
      "26369    25267                                      3           0   \n",
      "26370    25268                                      3           0   \n",
      "26371    25269                                      3           0   \n",
      "26372    25270                                      3           0   \n",
      "26373    25271                                      3           0   \n",
      "26374    25272                                      3           0   \n",
      "26375    25273                                      3           0   \n",
      "26376    25274                                      3           0   \n",
      "26377    25275                                      3           1   \n",
      "26378    25276                                      3           0   \n",
      "26379    25277                                      3           0   \n",
      "26380    25278                                      3           0   \n",
      "26381    25279                                      3           0   \n",
      "26382    25280                                      3           0   \n",
      "26383    25281                                      3           0   \n",
      "26384    25282                                      3           0   \n",
      "26385    25283                                      3           0   \n",
      "26386    25284                                      3           0   \n",
      "26387    25285                                      3           0   \n",
      "26388    25286                                      3           1   \n",
      "26389    25287                                      3           0   \n",
      "26390    25288                                      3           0   \n",
      "26391    25289                                      3           3   \n",
      "26392    25290                                      3           2   \n",
      "26393    25291                                      3           0   \n",
      "26394    25292                                      3           0   \n",
      "26395    25294                                      3           0   \n",
      "26396    25295                                      6           0   \n",
      "26397    25296                                      3           0   \n",
      "\n",
      "0     offensive_language neither class  \\\n",
      "1                      0       3     2   \n",
      "2                      3       0     1   \n",
      "3                      3       0     1   \n",
      "4                      2       1     1   \n",
      "5                      6       0     1   \n",
      "6                      2       0     1   \n",
      "7                      3       0     1   \n",
      "8                      3       0     1   \n",
      "9                      3       0     1   \n",
      "10                     2       0     1   \n",
      "11                  None    None  None   \n",
      "12                     3       0     1   \n",
      "13                     3       0     1   \n",
      "14                     2       1     1   \n",
      "15                     3       0     1   \n",
      "16                     2       0     1   \n",
      "17                     3       0     1   \n",
      "18                     3       0     1   \n",
      "19                     2       0     1   \n",
      "20                     3       0     1   \n",
      "21                     3       0     1   \n",
      "22                     3       0     1   \n",
      "23                     3       0     1   \n",
      "24                     3       0     1   \n",
      "25                     3       0     1   \n",
      "26                     3       0     1   \n",
      "27                     2       1     1   \n",
      "28                     3       0     1   \n",
      "29                     3       0     1   \n",
      "30                     3       0     1   \n",
      "...                  ...     ...   ...   \n",
      "26368                  2       0     1   \n",
      "26369                  3       0     1   \n",
      "26370                  3       0     1   \n",
      "26371                  3       0     1   \n",
      "26372                  3       0     1   \n",
      "26373                  3       0     1   \n",
      "26374                  3       0     1   \n",
      "26375                  2       1     1   \n",
      "26376                  3       0     1   \n",
      "26377                  2       0     1   \n",
      "26378                  2       1     1   \n",
      "26379                  3       0     1   \n",
      "26380                  3       0     1   \n",
      "26381                  3       0     1   \n",
      "26382                  1       2     2   \n",
      "26383                  3       0     1   \n",
      "26384                  3       0     1   \n",
      "26385                  3       0     1   \n",
      "26386                  3       0     1   \n",
      "26387                  3       0     1   \n",
      "26388                  2       0     1   \n",
      "26389                  3       0     1   \n",
      "26390                  3       0     1   \n",
      "26391                  0       0     0   \n",
      "26392                  1       0     0   \n",
      "26393                  2       1     1   \n",
      "26394                  1       2     2   \n",
      "26395                  3       0     1   \n",
      "26396                  6       0     1   \n",
      "26397                  0       3     2   \n",
      "\n",
      "0                                                  tweet  \\\n",
      "1      !!! RT @mayasolovely: As a woman you shouldn't...   \n",
      "2      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
      "3      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
      "4      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
      "5      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
      "6      !!!!!!!!!!!!!!!!!!@T_Madison_x: The shit just ...   \n",
      "7      !!!!!!@__BrighterDays: I can not just sit up a...   \n",
      "8      !!!!&#8220;@selfiequeenbri: cause I'm tired of...   \n",
      "9       &amp; you might not get ya bitch back &amp; t...   \n",
      "10         @rhythmixx_ :hobbies include: fighting Mariam   \n",
      "11                                                  None   \n",
      "12      Keeks is a bitch she curves everyone  lol I w...   \n",
      "13                       Murda Gang bitch its Gang Land    \n",
      "14      So hoes that smoke are losers ?  yea ... go o...   \n",
      "15            bad bitches is the only thing that i like    \n",
      "16                                  bitch get up off me    \n",
      "17                          bitch nigga miss me with it    \n",
      "18                                   bitch plz whatever    \n",
      "19                                bitch who do you love    \n",
      "20                       bitches get cut off everyday B    \n",
      "21                       black bottle &amp; a bad bitch    \n",
      "22                     broke bitch cant tell me nothing    \n",
      "23                          cancel that bitch like Nino    \n",
      "24                  cant you see these hoes wont change    \n",
      "25      fuck no that bitch dont even suck dick  &#128...   \n",
      "26      got ya bitch tip toeing on my hardwood floors...   \n",
      "27           her pussy lips like Heaven doors  &#128524;   \n",
      "28                             hoe what its hitting for    \n",
      "29      i met that pussy on Ocean Dr . i gave that pu...   \n",
      "30           i need a trippy bitch who fuck on Hennessy    \n",
      "...                                                  ...   \n",
      "26368             you ain't gotta be a dyke to like hoes   \n",
      "26369                                      you are a hoe   \n",
      "26370             you bitches love yall some corny nigga   \n",
      "26371  you can masturbate anytime bitch lol &#8220;@g...   \n",
      "26372  you can never get a group of hoes together wit...   \n",
      "26373    you can tell when dick recently been in a pussy   \n",
      "26374                          you can't cuff a hoe lmao   \n",
      "26375                         you drove me redneck crazy   \n",
      "26376                              you fake niggah lolol   \n",
      "26377                                     you got niggas   \n",
      "26378  you gotta be a new breed of retarded if you do...   \n",
      "26379  you gotta understand that these bitches are ch...   \n",
      "26380                                      you hoe spice   \n",
      "26381                   you just want some attention hoe   \n",
      "26382                             you know what they say   \n",
      "26383  you know what your doing when you favorite a t...   \n",
      "26384                             you lil dumb ass bitch   \n",
      "26385  you look like AC Green...bitch don't call here...   \n",
      "26386  you look like your 12 stop talking about fucki...   \n",
      "26387        you might as well gone pussy pop on a stage   \n",
      "26388              you niggers cheat on ya gf's? smh....   \n",
      "26389  you really care bout dis bitch. my dick all in...   \n",
      "26390                     you worried bout other bitches   \n",
      "26391                                 you're all niggers   \n",
      "26392  you're such a retard i hope you get type 2 dia...   \n",
      "26393  you's a muthaf***in lie &#8220;@LifeAsKing: @2...   \n",
      "26394         you've gone and broke the wrong heart baby   \n",
      "26395  young buck wanna eat!!.. dat nigguh like I ain...   \n",
      "26396              youu got wild bitches tellin you lies   \n",
      "26397  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...   \n",
      "\n",
      "0                                            clean_tweet  \n",
      "1      rt as a woman you shouldn t complain about cle...  \n",
      "2      rt boy dats cold tyga dwn bad for cuffin dat h...  \n",
      "3      rt dawg rt you ever fuck a bitch and she start...  \n",
      "4             rt g anderson based she look like a tranny  \n",
      "5      rt the shit you hear about me might be true or...  \n",
      "6      madison x the shit just blows me claim you so ...  \n",
      "7      brighterdays i can not just sit up and hate on...  \n",
      "8      8220 cause i m tired of you big bitches coming...  \n",
      "9      amp you might not get ya bitch back amp thats ...  \n",
      "10                                                  None  \n",
      "11                                                  None  \n",
      "12     keeks is a bitch she curves everyone lol i wal...  \n",
      "13                        murda gang bitch its gang land  \n",
      "14            so hoes that smoke are losers yea go on ig  \n",
      "15             bad bitches is the only thing that i like  \n",
      "16                                   bitch get up off me  \n",
      "17                           bitch nigga miss me with it  \n",
      "18                                    bitch plz whatever  \n",
      "19                                 bitch who do you love  \n",
      "20                        bitches get cut off everyday b  \n",
      "21                          black bottle amp a bad bitch  \n",
      "22                      broke bitch cant tell me nothing  \n",
      "23                           cancel that bitch like nino  \n",
      "24                   cant you see these hoes wont change  \n",
      "25     fuck no that bitch dont even suck dick 128514 ...  \n",
      "26     got ya bitch tip toeing on my hardwood floors ...  \n",
      "27               her pussy lips like heaven doors 128524  \n",
      "28                              hoe what its hitting for  \n",
      "29     i met that pussy on ocean dr i gave that pussy...  \n",
      "30            i need a trippy bitch who fuck on hennessy  \n",
      "...                                                  ...  \n",
      "26368             you ain t gotta be a dyke to like hoes  \n",
      "26369                                                hoe  \n",
      "26370             you bitches love yall some corny nigga  \n",
      "26371  you can masturbate anytime bitch lol 8220 i ju...  \n",
      "26372  you can never get a group of hoes together wit...  \n",
      "26373   taste nasty af ewww bitches better never play...  \n",
      "26374                          you can t cuff a hoe lmao  \n",
      "26375                         you drove me redneck crazy  \n",
      "26376                              you fake niggah lolol  \n",
      "26377                                 and i got bitches.  \n",
      "26378  you gotta be a new breed of retarded if you do...  \n",
      "26379  you gotta understand that these bitches are ch...  \n",
      "26380                                      you hoe spice  \n",
      "26381                   you just want some attention hoe  \n",
      "26382   the early bird gets the worm. *puts gummy wor...  \n",
      "26383                         tryna cheat ass bitch haha  \n",
      "26384          i ain't fuckin wit chu!.. i got a million  \n",
      "26385  you look like ac green bitch don t call here a...  \n",
      "26386  you look like your 12 stop talking about fucki...  \n",
      "26387        you might as well gone pussy pop on a stage  \n",
      "26388                   you niggers cheat on ya gf s smh  \n",
      "26389  you really care bout dis bitch my dick all in ...  \n",
      "26390                                   you need me for?  \n",
      "26391                                 you re all niggers  \n",
      "26392  you re such a retard i hope you get type 2 dia...  \n",
      "26393            mine? Bible scriptures and hymns&#8221;  \n",
      "26394                         and drove me redneck crazy  \n",
      "26395  young buck wanna eat dat nigguh like i aint fu...  \n",
      "26396              youu got wild bitches tellin you lies  \n",
      "26397                                             orange  \n",
      "\n",
      "[26397 rows x 8 columns]\n",
      "0     ï»¿index                                              count hate_speech  \\\n",
      "591        533                                                  3           0   \n",
      "592        534                                                  3           0   \n",
      "593        535                                                  3           2   \n",
      "594        536                                                  3           1   \n",
      "595        537                                                  3           0   \n",
      "596  &#128514;  my nips are freeeeezing and that s not okay be...        None   \n",
      "597        538                                                  3           0   \n",
      "598        539                                                  3           0   \n",
      "599        540                                                  3           2   \n",
      "600        541                                                  3           0   \n",
      "\n",
      "0   offensive_language neither class  \\\n",
      "591                  3       0     1   \n",
      "592                  1       2     2   \n",
      "593                  1       0     0   \n",
      "594                  2       0     1   \n",
      "595                  2       1     1   \n",
      "596               None    None  None   \n",
      "597                  3       0     1   \n",
      "598                  3       0     1   \n",
      "599                  1       0     0   \n",
      "600                  3       0     1   \n",
      "\n",
      "0                                                tweet  \\\n",
      "591  Maybe she wants to be more than your friend or...   \n",
      "592  Mine plays DMX and kirk Franklin smh @_justcal...   \n",
      "593  My grandma used to call me a porch monkey all ...   \n",
      "594  My momma told me to tell you to mind yoooo dam...   \n",
      "595  My nips are freeeeezing...and that's not okay ...   \n",
      "596                                               None   \n",
      "597  My twitter was hacked = I'm too pussy to admit...   \n",
      "598            Nah girl I ain't gon be able to make it   \n",
      "599  Nah its You @NoMeek_JustMilz: &#128514;&#12851...   \n",
      "600  New Flame the song of the yr lol that bitch SU...   \n",
      "\n",
      "0                                          clean_tweet  \n",
      "591  maybe she wants to be more than your friend or...  \n",
      "592  mine plays dmx and kirk franklin smh justcallm...  \n",
      "593  my grandma used to call me a porch monkey all ...  \n",
      "594  my momma told me to tell you to mind yoooo dam...  \n",
      "595                                               None  \n",
      "596                                               None  \n",
      "597  my twitter was hacked i m too pussy to admit t...  \n",
      "598   it's been tight since Russ loss his job you i...  \n",
      "599  nah its you justmilz 128514 128514 128514 1285...  \n",
      "600  new flame the song of the yr lol that bitch su...  \n",
      "(26397, 8)\n",
      "(23977, 8)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_tweets.csv',encoding='ISO-8859-1', sep='\\,', header=None, usecols=range(8)).apply(lambda x: x.str.replace(r\"\\\"\",\"\"))\n",
    "\n",
    "df.columns = df.iloc[0]\n",
    "\n",
    "df.drop(labels = 0, axis=0, inplace=True)\n",
    "print(df)\n",
    "print (df[590:600])\n",
    "print (df.shape)\n",
    "\n",
    "df = df.dropna() # still 24000 left of 26000, seems fine to me\n",
    "print (df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1380, 8)\n",
      "['rt', 'as', 'a', 'woman', 'you', 'shouldn', 't', 'complain', 'about', 'cleaning', 'up', 'your', 'house', 'amp', 'as', 'a', 'man', 'you', 'should', 'always', 'take', 'the', 'trash', 'out']\n",
      "['1' '1' '1' ... '0' '1' '2']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(df.loc[df['class'] == '0'].shape)\n",
    "\n",
    "\n",
    "classes = df['class'].values\n",
    "tweets_full = df['clean_tweet'].values\n",
    "tweets = []\n",
    "for tweet in tweets_full:\n",
    "    tweets.append(tweet.split())\n",
    "print(tweets[0])\n",
    "\n",
    "tweets_train, tweets_test, classes_train, classes_test = train_test_split(tweets, classes)\n",
    "print(classes_train)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(tweets_train)):\n",
    "    data = (tweets_train[i], classes_train[i])\n",
    "    train_data.append(data)\n",
    "\n",
    "test_data = []\n",
    "for i in range(len(tweets_test)):\n",
    "    data = (tweets_test[i], classes_test[i])\n",
    "    test_data.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 23916\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Here we first define a class that can map a word to an ID (w2i)\n",
    "# and back (i2w).\n",
    "\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
    "  def __repr__(self):\n",
    "    return '%s(%r)' % (self.__class__.__name__,\n",
    "                      OrderedDict(self))\n",
    "  def __reduce__(self):\n",
    "    return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.freqs = OrderedCounter()\n",
    "    self.w2i = {}\n",
    "    self.i2w = []\n",
    "\n",
    "  def count_token(self, t):\n",
    "    self.freqs[t] += 1\n",
    "    \n",
    "  def add_token(self, t):\n",
    "    self.w2i[t] = len(self.w2i)\n",
    "    self.i2w.append(t)    \n",
    "    \n",
    "  def build(self, min_freq=0):\n",
    "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
    "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)   \n",
    "    \n",
    "    tok_freq = list(self.freqs.items())\n",
    "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "    for tok, freq in tok_freq:\n",
    "      if freq >= min_freq:\n",
    "        self.add_token(tok)\n",
    "        \n",
    "# This process should be deterministic and should have the same result \n",
    "# if run multiple times on the same data set.\n",
    "\n",
    "v = Vocabulary()\n",
    "for tweet in tweets:\n",
    "    for word in tweet:\n",
    "        v.count_token(word)\n",
    "\n",
    "v.build()\n",
    "print(\"Vocabulary size:\", len(v.w2i))\n",
    "#print(v.i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(23916, 100)\n"
     ]
    }
   ],
   "source": [
    "embed = nn.Embedding(len(v.w2i), 100)\n",
    "print(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----GLOVE----\n",
      "[',', '-0.082752', '0.67204', '-0.14987', '-0.064983', '0.056491', '0.40228', '0.0027747', '-0.3311', '-0.30691', '2.0817', '0.031819', '0.013643', '0.30265', '0.0071297', '-0.5819', '-0.2774', '-0.062254', '1.1451', '-0.24232', '0.1235', '-0.12243', '0.33152', '-0.006162', '-0.30541', '-0.13057', '-0.054601', '0.037083', '-0.070552', '0.5893', '-0.30385', '0.2898', '-0.14653', '-0.27052', '0.37161', '0.32031', '-0.29125', '0.0052483', '-0.13212', '-0.052736', '0.087349', '-0.26668', '-0.16897', '0.015162', '-0.0083746', '-0.14871', '0.23413', '-0.20719', '-0.091386', '0.40075', '-0.17223', '0.18145', '0.37586', '-0.28682', '0.37289', '-0.16185', '0.18008', '0.3032', '-0.13216', '0.18352', '0.095759', '0.094916', '0.008289', '0.11761', '0.34046', '0.03677', '-0.29077', '0.058303', '-0.027814', '0.082941', '0.1862', '-0.031494', '0.27985', '-0.074412', '-0.13762', '-0.21866', '0.18138', '0.040855', '-0.113', '0.24107', '0.3657', '-0.27525', '-0.05684', '0.34872', '0.011884', '0.14517', '-0.71395', '0.48497', '0.14807', '0.62287', '0.20599', '0.58379', '-0.13438', '0.40207', '0.18311', '0.28021', '-0.42349', '-0.25626', '0.17715', '-0.54095', '0.16596', '-0.036058', '0.08499', '-0.64989', '0.075549', '-0.28831', '0.40626', '-0.2802', '0.094062', '0.32406', '0.28437', '-0.26341', '0.11553', '0.071918', '-0.47215', '-0.18366', '-0.34709', '0.29964', '-0.66514', '0.002516', '-0.42333', '0.27512', '0.36012', '0.16311', '0.23964', '-0.05923', '0.3261', '0.20559', '0.038677', '-0.045816', '0.089764', '0.43151', '-0.15954', '0.08532', '-0.26572', '-0.15001', '0.084286', '-0.16714', '-0.43004', '0.060807', '0.13121', '-0.24112', '0.66554', '0.4453', '-0.18019', '-0.13919', '0.56252', '0.21457', '-0.46443', '-0.012211', '0.029988', '-0.051094', '-0.20135', '0.80788', '0.47377', '-0.057647', '0.46216', '0.16084', '-0.20954', '-0.05452', '0.15572', '-0.13712', '0.12972', '-0.011936', '-0.003378', '-0.13595', '-0.080711', '0.20065', '0.054056', '0.046816', '0.059539', '0.046265', '0.17754', '-0.31094', '0.28119', '-0.24355', '0.085252', '-0.21011', '-0.19472', '0.0027297', '-0.46341', '0.14789', '-0.31517', '-0.065939', '0.036106', '0.42903', '-0.33759', '0.16432', '0.32568', '-0.050392', '-0.054297', '0.24074', '0.41923', '0.13012', '-0.17167', '-0.37808', '-0.23089', '-0.019477', '-0.29291', '-0.30824', '0.30297', '-0.22659', '0.081574', '-0.18516', '-0.21408', '0.40616', '-0.28974', '0.074174', '-0.17795', '0.28595', '-0.039626', '-0.2339', '-0.36054', '-0.067503', '-0.091065', '0.23438', '-0.0041331', '0.003232', '0.0072134', '0.008697', '0.21614', '0.049904', '0.35582', '0.13748', '0.073361', '0.14166', '0.2412', '-0.013322', '0.15613', '0.083381', '0.088146', '-0.019357', '0.43795', '0.083961', '0.45309', '-0.50489', '-0.10865', '-0.2527', '-0.18251', '0.20441', '0.13319', '0.1294', '0.050594', '-0.15612', '-0.39543', '0.12538', '0.24881', '-0.1927', '-0.31847', '-0.12719', '0.4341', '0.31177', '-0.0040946', '-0.2094', '-0.079961', '0.1161', '-0.050794', '0.015266', '-0.2803', '-0.12486', '0.23587', '0.2339', '-0.14023', '0.028462', '0.56923', '-0.1649', '-0.036429', '0.010051', '-0.17107', '-0.042608', '0.044965', '-0.4393', '-0.26137', '0.30088', '-0.060772', '-0.45312', '-0.19076', '-0.20288', '0.27694', '-0.060888', '0.11944', '0.62206', '-0.19343', '0.47849', '-0.30113', '0.059389', '0.074901', '0.061068', '-0.4662', '0.40054', '-0.19099', '-0.14331', '0.018267', '-0.18643', '0.20709', '-0.35598', '0.05338', '-0.050821', '-0.1918', '-0.37846', '-0.06589']\n",
      "['.', '0.012001', '0.20751', '-0.12578', '-0.59325', '0.12525', '0.15975', '0.13748', '-0.33157', '-0.13694', '1.7893', '-0.47094', '0.70434', '0.26673', '-0.089961', '-0.18168', '0.067226', '0.053347', '1.5595', '-0.2541', '0.038413', '-0.01409', '0.056774', '0.023434', '0.024042', '0.31703', '0.19025', '-0.37505', '0.035603', '0.1181', '0.012032', '-0.037566', '-0.5046', '-0.049261', '0.092351', '0.11031', '-0.073062', '0.33994', '0.28239', '0.13413', '0.070128', '-0.022099', '-0.28103', '0.49607', '-0.48693', '-0.090964', '-0.1538', '-0.38011', '-0.014228', '-0.19392', '-0.11068', '-0.014088', '-0.17906', '0.24509', '-0.16878', '-0.15351', '-0.13808', '0.02151', '0.13699', '0.0068061', '-0.14915', '-0.38169', '0.12727', '0.44007', '0.32678', '-0.46117', '0.068687', '0.34747', '0.18827', '-0.31837', '0.4447', '-0.2095', '-0.26987', '0.48945', '0.15388', '0.05295', '-0.049831', '0.11207', '0.14881', '-0.37003', '0.30777', '-0.33865', '0.045149', '-0.18987', '0.26634', '-0.26401', '-0.47556', '0.68381', '-0.30653', '0.24606', '0.31611', '-0.071098', '0.030417', '0.088119', '0.045025', '0.20125', '-0.21618', '-0.36371', '-0.25948', '-0.42398', '-0.14305', '-0.10208', '0.21498', '-0.21924', '-0.17935', '0.21546', '0.13801', '0.24504', '-0.2559', '0.054815', '0.21307', '0.2564', '-0.25673', '0.17961', '-0.47638', '-0.25181', '-0.0091498', '-0.054362', '-0.21007', '0.12597', '-0.40795', '-0.021164', '0.20585', '0.18925', '-0.0051896', '-0.51394', '0.28862', '-0.077748', '-0.27676', '0.46567', '-0.14225', '-0.17879', '-0.4357', '-0.32481', '0.15034', '-0.058367', '0.49652', '0.20472', '0.019866', '0.13326', '0.12823', '-1.0177', '0.29007', '0.28995', '0.029994', '-0.10763', '0.28665', '-0.24387', '0.22905', '-0.26249', '-0.069269', '-0.17889', '0.21936', '0.15146', '0.04567', '-0.050497', '0.071482', '-0.1027', '-0.080705', '0.30296', '0.031302', '0.26613', '-0.0060951', '0.10313', '-0.39987', '-0.043945', '-0.057625', '0.08702', '-0.098152', '0.22835', '-0.005211', '0.038075', '0.01591', '-0.20622', '0.021853', '0.0040426', '-0.043063', '-0.002294', '-0.26097', '-0.25802', '-0.28158', '-0.23118', '-0.010404', '-0.30102', '-0.4042', '0.014653', '-0.10445', '0.30377', '-0.20957', '0.3119', '0.068272', '0.1008', '0.010423', '0.54011', '0.29865', '0.12653', '0.013761', '0.21738', '-0.39521', '0.066633', '0.50327', '0.14913', '-0.11554', '0.010042', '0.095698', '0.16607', '-0.18808', '0.055019', '0.026715', '-0.3164', '-0.046583', '-0.051591', '0.023475', '-0.11007', '0.085642', '0.28394', '0.040497', '0.071986', '0.14157', '-0.021199', '0.44718', '0.20088', '-0.12964', '-0.067183', '0.47614', '0.13394', '-0.17287', '-0.37324', '-0.17285', '0.02683', '-0.1316', '0.09116', '-0.46487', '0.1274', '-0.090159', '-0.10552', '0.068006', '-0.13381', '0.17056', '0.089509', '-0.23133', '-0.27572', '0.061534', '-0.051646', '0.28377', '0.25286', '-0.24139', '-0.19905', '0.12049', '-0.1011', '0.27392', '0.27843', '0.26449', '-0.18292', '-0.048961', '0.19198', '0.17192', '0.33659', '-0.20184', '-0.34305', '-0.24553', '-0.15399', '0.3945', '0.22839', '-0.25753', '-0.25675', '-0.37332', '-0.23884', '-0.048816', '0.78323', '0.18851', '-0.26477', '0.096566', '0.062658', '-0.30668', '-0.43334', '0.10006', '0.21136', '0.039459', '-0.11077', '0.24421', '0.60942', '-0.46646', '0.086385', '-0.39702', '-0.23363', '0.021307', '-0.10778', '-0.2281', '0.50803', '0.11567', '0.16165', '-0.066737', '-0.29556', '0.022612', '-0.28135', '0.0635', '0.14019', '0.13871', '-0.36049', '-0.035']\n",
      "['the', '0.27204', '-0.06203', '-0.1884', '0.023225', '-0.018158', '0.0067192', '-0.13877', '0.17708', '0.17709', '2.5882', '-0.35179', '-0.17312', '0.43285', '-0.10708', '0.15006', '-0.19982', '-0.19093', '1.1871', '-0.16207', '-0.23538', '0.003664', '-0.19156', '-0.085662', '0.039199', '-0.066449', '-0.04209', '-0.19122', '0.011679', '-0.37138', '0.21886', '0.0011423', '0.4319', '-0.14205', '0.38059', '0.30654', '0.020167', '-0.18316', '-0.0065186', '-0.0080549', '-0.12063', '0.027507', '0.29839', '-0.22896', '-0.22882', '0.14671', '-0.076301', '-0.1268', '-0.0066651', '-0.052795', '0.14258', '0.1561', '0.05551', '-0.16149', '0.09629', '-0.076533', '-0.049971', '-0.010195', '-0.047641', '-0.16679', '-0.2394', '0.0050141', '-0.049175', '0.013338', '0.41923', '-0.10104', '0.015111', '-0.077706', '-0.13471', '0.119', '0.10802', '0.21061', '-0.051904', '0.18527', '0.17856', '0.041293', '-0.014385', '-0.082567', '-0.035483', '-0.076173', '-0.045367', '0.089281', '0.33672', '-0.22099', '-0.0067275', '0.23983', '-0.23147', '-0.88592', '0.091297', '-0.012123', '0.013233', '-0.25799', '-0.02972', '0.016754', '0.01369', '0.32377', '0.039546', '0.042114', '-0.088243', '0.30318', '0.087747', '0.16346', '-0.40485', '-0.043845', '-0.040697', '0.20936', '-0.77795', '0.2997', '0.2334', '0.14891', '-0.39037', '-0.053086', '0.062922', '0.065663', '-0.13906', '0.094193', '0.10344', '-0.2797', '0.28905', '-0.32161', '0.020687', '0.063254', '-0.23257', '-0.4352', '-0.017049', '-0.32744', '-0.047064', '-0.075149', '-0.18788', '-0.015017', '0.029342', '-0.3527', '-0.044278', '-0.13507', '-0.11644', '-0.1043', '0.1392', '0.0039199', '0.37603', '0.067217', '-0.37992', '-1.1241', '-0.057357', '-0.16826', '0.03941', '0.2604', '-0.023866', '0.17963', '0.13553', '0.2139', '0.052633', '-0.25033', '-0.11307', '0.22234', '0.066597', '-0.11161', '0.062438', '-0.27972', '0.19878', '-0.36262', '-1.0006e-05', '-0.17262', '0.29166', '-0.15723', '0.054295', '0.06101', '-0.39165', '0.2766', '0.057816', '0.39709', '0.025229', '0.24672', '-0.08905', '0.15683', '-0.2096', '-0.22196', '0.052394', '-0.01136', '0.050417', '-0.14023', '-0.042825', '-0.031931', '-0.21336', '-0.20402', '-0.23272', '0.07449', '0.088202', '-0.11063', '-0.33526', '-0.014028', '-0.29429', '-0.086911', '-0.1321', '-0.43616', '0.20513', '0.0079362', '0.48505', '0.064237', '0.14261', '-0.43711', '0.12783', '-0.13111', '0.24673', '-0.27496', '0.15896', '0.43314', '0.090286', '0.24662', '0.066463', '-0.20099', '0.1101', '0.03644', '0.17359', '-0.15689', '-0.086328', '-0.17316', '0.36975', '-0.40317', '-0.064814', '-0.034166', '-0.013773', '0.062854', '-0.17183', '-0.12366', '-0.034663', '-0.22793', '-0.23172', '0.239', '0.27473', '0.15332', '0.10661', '-0.060982', '-0.024805', '-0.13478', '0.17932', '-0.37374', '-0.02893', '-0.11142', '-0.08389', '-0.055932', '0.068039', '-0.10783', '0.1465', '0.094617', '-0.084554', '0.067429', '-0.3291', '0.034082', '-0.16747', '-0.25997', '-0.22917', '0.020159', '-0.02758', '0.16136', '-0.18538', '0.037665', '0.57603', '0.20684', '0.27941', '0.16477', '-0.018769', '0.12062', '0.069648', '0.059022', '-0.23154', '0.24095', '-0.3471', '0.04854', '-0.056502', '0.41566', '-0.43194', '0.4823', '-0.051759', '-0.27285', '-0.25893', '0.16555', '-0.1831', '-0.06734', '0.42457', '0.010346', '0.14237', '0.25939', '0.17123', '-0.13821', '-0.066846', '0.015981', '-0.30193', '0.043579', '-0.043102', '0.35025', '-0.19681', '-0.4281', '0.16899', '0.22511', '-0.28557', '-0.1028', '-0.018168', '0.11407', '0.13015', '-0.18317', '0.1323']\n",
      "['and', '-0.18567', '0.066008', '-0.25209', '-0.11725', '0.26513', '0.064908', '0.12291', '-0.093979', '0.024321', '2.4926', '-0.017916', '-0.071218', '-0.24782', '-0.26237', '-0.2246', '-0.21961', '-0.12927', '1.0867', '-0.66072', '-0.031617', '-0.057328', '0.056903', '-0.27939', '-0.39825', '0.14251', '-0.085146', '-0.14779', '0.055067', '-0.0028687', '-0.20917', '-0.070735', '0.22577', '-0.15881', '-0.10395', '0.09711', '-0.56251', '-0.32929', '-0.20853', '0.0098711', '0.049777', '0.0014883', '0.15884', '0.042771', '-0.0026956', '-0.02462', '-0.19213', '-0.22556', '0.10838', '0.090086', '-0.13291', '0.32559', '-0.17038', '-0.1099', '-0.23986', '-0.024289', '0.014656', '-0.237', '0.084828', '-0.35982', '-0.076746', '0.048909', '0.11431', '-0.21013', '0.24765', '-0.017531', '-0.14028', '0.046191', '0.22972', '0.1175', '0.12724', '0.012992', '0.4587', '0.41085', '0.039106', '0.15713', '-0.18376', '0.26834', '0.056662', '0.16844', '-0.053788', '-0.091892', '0.11193', '-0.08681', '-0.13324', '0.15062', '-0.31733', '-0.22078', '0.25038', '0.34131', '0.36419', '-0.089514', '-0.22193', '0.24471', '0.040091', '0.47798', '-0.029996', '0.0019212', '0.063511', '-0.20417', '-0.26478', '0.20649', '0.015573', '-0.27722', '-0.18861', '-0.10289', '-0.49773', '0.14986', '-0.010877', '0.25085', '-0.28117', '0.18966', '-0.065879', '0.094753', '-0.15338', '-0.055071', '-0.36747', '0.24993', '0.096527', '0.23538', '0.18405', '0.052859', '0.22967', '0.12582', '0.15536', '-0.17275', '0.33946', '-0.10049', '0.074948', '-0.093575', '-0.04049', '-0.016922', '-0.0058039', '-0.18108', '0.19537', '0.45178', '0.10965', '0.2337', '-0.09905', '-0.078633', '0.21678', '-0.71231', '-0.099759', '0.33333', '-0.1646', '-0.091688', '0.21056', '0.023669', '0.028922', '0.1199', '-0.12512', '-0.026037', '-0.062217', '0.55816', '0.0050273', '-0.30888', '0.038611', '0.17568', '-0.11163', '-0.10815', '-0.19444', '0.29433', '0.14519', '-0.042878', '0.18534', '0.018891', '-0.61883', '0.13352', '0.036007', '0.33995', '0.22109', '-0.079328', '0.071319', '0.17678', '0.16378', '-0.23142', '-0.1434', '-0.098122', '-0.019286', '0.2356', '-0.34013', '-0.061007', '-0.23208', '-0.31152', '0.10063', '-0.15957', '0.20183', '-0.016345', '-0.12303', '0.022667', '-0.20986', '-0.20127', '-0.087883', '0.064731', '0.10195', '-0.1786', '0.33056', '0.21407', '-0.32165', '-0.17106', '0.19407', '-0.38618', '-0.2148', '-0.052254', '0.023175', '0.47389', '0.18612', '0.12711', '0.20855', '-0.10256', '-0.12016', '-0.40488', '0.029695', '-0.027419', '-0.0085227', '-0.11415', '0.081134', '-0.17228', '0.19142', '0.026514', '0.043789', '-0.12399', '0.13354', '0.10112', '0.081682', '-0.15085', '0.0075806', '-0.18971', '0.24669', '0.22491', '0.35553', '-0.3277', '-0.21821', '0.1402', '0.28604', '0.055226', '-0.086544', '0.02111', '-0.19236', '0.074245', '0.076782', '0.00081666', '0.034097', '-0.57719', '0.10657', '0.28134', '-0.11964', '-0.68281', '-0.32893', '-0.24442', '-0.025847', '0.0091273', '0.2025', '-0.050959', '-0.11042', '0.010962', '0.076773', '0.40048', '-0.40739', '-0.44773', '0.31954', '-0.036326', '-0.012789', '-0.17282', '0.1476', '0.2356', '0.080642', '-0.36528', '-0.0083443', '0.6239', '-0.24379', '0.019917', '-0.28803', '-0.010494', '0.038412', '-0.11718', '-0.072462', '0.16381', '0.38488', '-0.029783', '0.23444', '0.4532', '0.14815', '-0.027021', '-0.073181', '-0.1147', '-0.0054545', '0.47796', '0.090912', '0.094489', '-0.36882', '-0.59396', '-0.097729', '0.20072', '0.17055', '-0.0047356', '-0.039709', '0.32498', '-0.023452', '0.12302', '0.3312']\n",
      "23916\n",
      "(23916, 300)\n"
     ]
    }
   ],
   "source": [
    "## GLOVE DATA\n",
    "\n",
    "glove_data = []\n",
    "\n",
    "print(\"----GLOVE----\")\n",
    "line_number = 0\n",
    "with open('../data/glove.txt', mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        glove_data.append(line)\n",
    "        if (line_number < 4):\n",
    "            print(line)\n",
    "            line_number += 1\n",
    "\n",
    "v_glove_dic = {}\n",
    "v_glove_dic['<unk>'] = np.random.rand(len(glove_data[0][1:]))\n",
    "v_glove_dic['<pad>'] = np.zeros(len(glove_data[0][1:]))\n",
    "\n",
    "for line in glove_data:\n",
    "    v_glove_dic[line[0]] = line[1:]\n",
    "\n",
    "\n",
    "\n",
    "v_glove = Vocabulary()\n",
    "for tweet in tweets:\n",
    "    for word in tweet:\n",
    "        v_glove.count_token(word)\n",
    "      \n",
    "\n",
    "v_glove.build()\n",
    "\n",
    "\n",
    "vectors = []\n",
    "counter = 0\n",
    "for line in v_glove.i2w:\n",
    "    if line in v_glove_dic and len(v_glove_dic[line]) == 300:\n",
    "        vectors.append(v_glove_dic[line])\n",
    "    else:\n",
    "        vectors.append(np.random.rand(len(glove_data[0][1:])))\n",
    "\n",
    "print(len(vectors))\n",
    "vectors = np.stack(vectors, axis=0).astype(np.float)\n",
    "print(vectors.shape)\n",
    "\n",
    "# copy pre-trained word vectors into embeddings table\n",
    "# model.embed.weight.data.copy_(torch.from_numpy(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FUNCTIONS FOR LSTM TRAINING. A COUPLE ARE JUST 1-BATCH LSTM, SO, UNNECESSARY\n",
    "\n",
    "# Here we print each parameter name, shape, and if it is trainable.\n",
    "def print_parameters(model):\n",
    "  total = 0\n",
    "  for name, p in model.named_parameters():\n",
    "    total += np.prod(p.shape)\n",
    "    print(\"{:24s} {:12s} requires_grad={}\".format(name, str(list(p.shape)), p.requires_grad))\n",
    "  print(\"\\nTotal parameters: {}\\n\".format(total))\n",
    "\n",
    "def prepare_example(example, vocab):\n",
    "  \"\"\"\n",
    "  Map tokens to their IDs for 1 example\n",
    "  \"\"\"\n",
    "  tweet, clas = example\n",
    "  # vocab returns 0 if the word is not there\n",
    "  x = [vocab.w2i.get(t, 0) for t in tweet]\n",
    "  \n",
    "  x = torch.LongTensor([x])\n",
    "  x = x.to(device)\n",
    "  \n",
    "  y = torch.LongTensor([float(clas)])\n",
    "  y = y.to(device)\n",
    "  \n",
    "  return x, y\n",
    "\n",
    "\n",
    "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
    "  \"\"\"Accuracy of a model on given data set.\"\"\"\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()  # disable dropout (explained later)\n",
    "\n",
    "  for example in data:\n",
    "    \n",
    "    # convert the example input and label to PyTorch tensors\n",
    "    x, target = prep_fn(example, model.vocab)\n",
    "\n",
    "    # forward pass\n",
    "    # get the output from the neural network for input x\n",
    "    with torch.no_grad():\n",
    "      logits = model(x)\n",
    "    \n",
    "    # get out the prediction\n",
    "    prediction = logits.argmax(dim=-1)\n",
    "    \n",
    "    # add the number of correct predictions to the total correct\n",
    "    correct += (prediction == target).sum().item()\n",
    "    total += 1\n",
    "\n",
    "  return correct, total, correct / float(total)\n",
    "\n",
    "def get_examples(data, shuffle=True, **kwargs):\n",
    "  \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
    "  if shuffle:\n",
    "    print(\"Shuffling training data\")\n",
    "    random.shuffle(data)  # shuffle training data each epoch\n",
    "  for example in data:\n",
    "    yield example\n",
    "    \n",
    "def train_model(model, optimizer, num_iterations=1000, \n",
    "                print_every=100, eval_every=100,\n",
    "                batch_fn=get_examples, \n",
    "                prep_fn=prepare_example,\n",
    "                eval_fn=simple_evaluate,\n",
    "                batch_size=1, eval_batch_size=None):\n",
    "  \"\"\"Train a model.\"\"\"  \n",
    "  iter_i = 0\n",
    "  train_loss = 0.\n",
    "  print_num = 0\n",
    "  start = time.time()\n",
    "  criterion = nn.CrossEntropyLoss() # loss function\n",
    "  best_eval = 0.\n",
    "  best_iter = 0\n",
    "  \n",
    "  # store train loss and validation accuracy during training\n",
    "  # so we can plot them afterwards\n",
    "  losses = []\n",
    "  accuracies = []  \n",
    "  \n",
    "  if eval_batch_size is None:\n",
    "    eval_batch_size = batch_size\n",
    "  while True:  # when we run out of examples, shuffle and continue\n",
    "    for batch in batch_fn(train_data, batch_size=batch_size):\n",
    "      # forward pass\n",
    "      model.train()\n",
    "      x, targets = prep_fn(batch, model.vocab)\n",
    "\n",
    "      logits = model(x)\n",
    "\n",
    "      B = targets.size(0)  # later we will use B examples per update\n",
    "      \n",
    "      # compute cross-entropy loss (our criterion)\n",
    "      # note that the cross entropy loss function computes the softmax for us\n",
    "      loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
    "      train_loss += loss.item()\n",
    "      # backward pass\n",
    "      # Tip: check the Introduction to PyTorch notebook.\n",
    "\n",
    "      # reset gradients\n",
    "      model.zero_grad()\n",
    "\n",
    "      # compute gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # update weights - take a small step in the opposite dir of the gradient\n",
    "      optimizer.step()\n",
    "\n",
    "      print_num += 1\n",
    "      iter_i += 1\n",
    "\n",
    "      # print info\n",
    "      if iter_i % print_every == 0:\n",
    "        print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
    "              (iter_i, train_loss, time.time()-start))\n",
    "        losses.append(train_loss)\n",
    "        print_num = 0        \n",
    "        train_loss = 0.\n",
    "\n",
    "      # evaluate\n",
    "      if iter_i % eval_every == 0:\n",
    "        _, _, accuracy_test = eval_fn(model, test_data, batch_size=eval_batch_size,\n",
    "                         batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "        accuracies.append(accuracy_test)\n",
    "        print(\"iter %r: test acc=%.4f\" % (iter_i, accuracy_test))       \n",
    "        \n",
    "        # save best model parameters\n",
    "        if accuracy_test > best_eval:\n",
    "          print(\"new highscore\")\n",
    "          best_eval = accuracy_test\n",
    "          best_iter = iter_i\n",
    "          path = \"{}.pt\".format(model.__class__.__name__)\n",
    "          ckpt = {\n",
    "              \"state_dict\": model.state_dict(),\n",
    "              \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "              \"best_eval\": best_eval,\n",
    "              \"best_iter\": best_iter\n",
    "          }\n",
    "          torch.save(ckpt, path)\n",
    "\n",
    "      # done training\n",
    "      if iter_i == num_iterations:\n",
    "        print(\"Done training\")\n",
    "        \n",
    "        # evaluate on train, dev, and test with best model\n",
    "        print(\"Loading best model\")\n",
    "        path = \"{}.pt\".format(model.__class__.__name__)        \n",
    "        ckpt = torch.load(path)\n",
    "        model.load_state_dict(ckpt[\"state_dict\"])\n",
    "        \n",
    "        _, _, train_acc = eval_fn(\n",
    "            model, train_data, batch_size=eval_batch_size, \n",
    "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "        _, _, test_acc = eval_fn(\n",
    "            model, test_data, batch_size=eval_batch_size, \n",
    "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
    "        \n",
    "        print(\"best model iter {:d}: \"\n",
    "              \"train_acc={:.4f}, test acc={:.4f}\".format(\n",
    "                  best_iter, train_acc, test_acc))\n",
    "        \n",
    "        return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMCell(nn.Module):\n",
    "  \"\"\"Our own LSTM cell\"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
    "    super(MyLSTMCell, self).__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "    \n",
    "    self.W_i = nn.Linear(input_size, 4*hidden_size, bias = self.bias)\n",
    "    self.W_h = nn.Linear(hidden_size, 4*hidden_size, bias = self.bias)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    self.reset_parameters()\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
    "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "    for weight in self.parameters():\n",
    "      weight.data.uniform_(-stdv, stdv)  \n",
    "\n",
    "  def forward(self, input_, hx, mask=None):\n",
    "    \"\"\"\n",
    "    input is (batch, input_size)\n",
    "    hx is ((batch, hidden_size), (batch, hidden_size))\n",
    "    \"\"\"\n",
    "    prev_h, prev_c = hx\n",
    "\n",
    "    # project input and prev state\n",
    "    x = input_\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    total = (self.W_i(x) + self.W_h(prev_h))\n",
    "    i,f,g,o = torch.chunk(total, 4 , dim=1)\n",
    "    \n",
    "    # main LSTM computation    \n",
    "    i = i.sigmoid()\n",
    "    f = f.sigmoid()\n",
    "    g = g.tanh()\n",
    "    o = o.sigmoid()\n",
    "\n",
    "    c = f*prev_c + i * g\n",
    "    h = o * c.tanh()\n",
    "\n",
    "    return h, c\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return \"{}({:d}, {:d})\".format(\n",
    "        self.__class__.__name__, self.input_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MINI BATCH LSTM\n",
    "\n",
    "def get_minibatch(data, batch_size=25, shuffle=True):\n",
    "  \"\"\"Return minibatches, optional shuffling\"\"\"\n",
    "  \n",
    "  if shuffle:\n",
    "    print(\"Shuffling training data\")\n",
    "    random.shuffle(data)  # shuffle training data each epoch\n",
    "  \n",
    "  batch = []\n",
    "  \n",
    "  # yield minibatches\n",
    "  for example in data:\n",
    "    batch.append(example)\n",
    "    \n",
    "    if len(batch) == batch_size:\n",
    "      yield batch\n",
    "      batch = []\n",
    "      \n",
    "  # in case there is something left\n",
    "  if len(batch) > 0:\n",
    "    yield batch\n",
    "    \n",
    "def pad(tokens, length, pad_value=1):\n",
    "  \"\"\"add padding 1s to a sequence to that it has the desired length\"\"\"\n",
    "  return tokens + [pad_value] * (length - len(tokens))\n",
    "\n",
    "def prepare_minibatch(mb, vocab):\n",
    "  \"\"\"\n",
    "  Minibatch is a list of examples.\n",
    "  This function converts words to IDs and returns\n",
    "  torch tensors to be used as input/targets.\n",
    "  \"\"\"\n",
    "  batch_size = len(mb)\n",
    "  maxlen = max([len(ex) for (ex, clas) in mb])\n",
    "  # vocab returns 0 if the word is not there\n",
    "  x = [pad([vocab.w2i.get(t, 0) for t in ex], maxlen) for (ex,_) in mb]\n",
    "  \n",
    "  x = torch.LongTensor(x)\n",
    "  x = x.to(device)\n",
    "  \n",
    "  y = [float(clas) for ex,clas in mb]\n",
    "  y = torch.LongTensor(y)\n",
    "  y = y.to(device)\n",
    "  \n",
    "  return x, y\n",
    "\n",
    "def evaluate(model, data, \n",
    "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
    "             batch_size=16):\n",
    "  \"\"\"Accuracy of a model on given data set (using minibatches)\"\"\"\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()  # disable dropout\n",
    "  \n",
    "  for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
    "    x, targets = prep_fn(mb, model.vocab)\n",
    "    with torch.no_grad():\n",
    "      logits = model(x)\n",
    "    predictions = logits.argmax(dim=-1).view(-1)\n",
    "    \n",
    "    # add the number of correct predictions to the total correct\n",
    "    correct += (predictions == targets.view(-1)).sum().item()\n",
    "    total += targets.size(0)\n",
    "\n",
    "  return correct, total, correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "  \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
    "    super(LSTMClassifier, self).__init__()\n",
    "    self.vocab = vocab\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "    self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
    "    \n",
    "    self.output_layer = nn.Sequential(     \n",
    "        nn.Dropout(p=0.5),  # explained later\n",
    "        nn.Linear(hidden_dim, output_dim)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "    B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
    "    T = x.size(1)  # time (the number of words in the sentence)\n",
    "    \n",
    "    input_ = self.embed(x)\n",
    "\n",
    "    # here we create initial hidden states containing zeros\n",
    "    # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
    "    hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "    cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
    "    \n",
    "    # process input sentences one word/timestep at a time\n",
    "    # input is batch-major, so the first word(s) is/are input_[:, 0]\n",
    "    outputs = []   \n",
    "    for i in range(T):\n",
    "      hx, cx = self.rnn(input_[:, i], (hx, cx))\n",
    "      outputs.append(hx)\n",
    "    \n",
    "    # if we have a single example, our final LSTM state is the last hx\n",
    "    if B == 1:\n",
    "      final = hx\n",
    "    else:\n",
    "      #\n",
    "      # This part is explained in next section, ignore this else-block for now.\n",
    "      #\n",
    "      # we processed sentences with different lengths, so some of the sentences\n",
    "      # had already finished and we have been adding padding inputs to hx\n",
    "      # we select the final state based on the length of each sentence\n",
    "      \n",
    "      # two lines below not needed if using LSTM form pytorch\n",
    "      outputs = torch.stack(outputs, dim=0)          # [T, B, D]\n",
    "      outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
    "      \n",
    "      # to be super-sure we're not accidentally indexing the wrong state\n",
    "      # we zero out positions that are invalid\n",
    "      pad_positions = (x == 1).unsqueeze(-1)\n",
    "      \n",
    "      outputs = outputs.contiguous()      \n",
    "      outputs = outputs.masked_fill_(pad_positions, 0.)\n",
    "        \n",
    "      mask = (x != 1)  # true for valid positions [B, T]\n",
    "      lengths = mask.sum(dim=1)                  # [B, 1]\n",
    "\n",
    "      indexes = (lengths - 1) + torch.arange(B, device=x.device, dtype=x.dtype) * T\n",
    "      final = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
    "    \n",
    "    # we use the last hidden state to classify the sentence\n",
    "    logits = self.output_layer(final)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embed): Embedding(23916, 300, padding_idx=1)\n",
      "  (rnn): MyLSTMCell(300, 168)\n",
      "  (output_layer): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=168, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [23916, 300] requires_grad=True\n",
      "rnn.W_i.weight           [672, 300]   requires_grad=True\n",
      "rnn.W_i.bias             [672]        requires_grad=True\n",
      "rnn.W_h.weight           [672, 168]   requires_grad=True\n",
      "rnn.W_h.bias             [672]        requires_grad=True\n",
      "output_layer.1.weight    [3, 168]     requires_grad=True\n",
      "output_layer.1.bias      [3]          requires_grad=True\n",
      "\n",
      "Total parameters: 7491147\n",
      "\n",
      "Shuffling training data\n",
      "Iter 100: loss=85.8513, time=36.34s\n",
      "Iter 200: loss=58.1558, time=71.56s\n",
      "Iter 300: loss=53.1827, time=107.25s\n",
      "Iter 400: loss=48.4333, time=141.57s\n",
      "Iter 500: loss=43.4565, time=176.25s\n",
      "Iter 600: loss=48.0554, time=211.71s\n",
      "Iter 700: loss=45.5909, time=246.95s\n",
      "Shuffling training data\n",
      "Iter 800: loss=40.9338, time=287.65s\n",
      "Iter 900: loss=38.7472, time=335.08s\n",
      "Iter 1000: loss=39.0177, time=378.30s\n",
      "iter 1000: test acc=0.0244\n",
      "new highscore\n",
      "Iter 1100: loss=36.9359, time=429.17s\n",
      "Iter 1200: loss=36.5718, time=469.94s\n",
      "Iter 1300: loss=40.0581, time=509.43s\n",
      "Iter 1400: loss=34.4489, time=546.39s\n",
      "Shuffling training data\n",
      "Iter 1500: loss=33.2953, time=585.10s\n",
      "Iter 1600: loss=31.8767, time=628.35s\n",
      "Iter 1700: loss=31.7997, time=671.00s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-eaa2c37564c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mbatch_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_minibatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprep_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprepare_minibatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     eval_fn=evaluate)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-784e017cb946>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m       \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m       \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[1;31m# update weights - take a small step in the opposite dir of the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jan_h_000\\miniconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jan_h_000\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "lstm_model = LSTMClassifier(len(v.w2i), 300, 168, 3, v)\n",
    "\n",
    "# IF YOU WANT GLOVE IN THE LSTM, UNCOMMENT THIS\n",
    "# with torch.no_grad():\n",
    "#     lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "#     lstm_model.embed.weight.requires_grad = True\n",
    "\n",
    "print(lstm_model)\n",
    "print_parameters(lstm_model)\n",
    "\n",
    "batch_size = 25\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=2e-4)\n",
    "\n",
    "lstm_losses, lstm_accuracies = train_model(\n",
    "    lstm_model, optimizer, num_iterations=6000, \n",
    "    print_every=100, eval_every=1000,\n",
    "    batch_size=batch_size,\n",
    "    batch_fn=get_minibatch, \n",
    "    prep_fn=prepare_minibatch,\n",
    "    eval_fn=evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
